# coding=utf-8
"""
Improved GNN dengan proper handling untuk 3-class classification
"""
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, classification_report, f1_score
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle
from sklearn.utils.class_weight import compute_class_weight

# Load data
print("Loading data...")
with open('./absa_data/twitter2015/processed_data.pkl', 'rb') as f:
    data = pickle.load(f)

# Improved Model dengan better architecture
class ImprovedGNN(nn.Module):
    def __init__(self, input_dim=300, hidden1=512, hidden2=256, hidden3=128, output_dim=3, dropout=0.3):
        super(ImprovedGNN, self).__init__()
        # Larger network dengan better regularization
        self.fc1 = nn.Linear(input_dim, hidden1)
        self.bn1 = nn.BatchNorm1d(hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.bn2 = nn.BatchNorm1d(hidden2)
        self.fc3 = nn.Linear(hidden2, hidden3)
        self.bn3 = nn.BatchNorm1d(hidden3)
        self.fc4 = nn.Linear(hidden3, output_dim)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)
        x = self.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = self.relu(self.bn3(self.fc3(x)))
        x = self.dropout(x)
        x = self.fc4(x)
        return x

# Prepare features dengan random embeddings yang lebih bermakna
print("Preparing features...")
import torch.nn.functional as F

# Use random embeddings dengan normalize untuk stabilitas
np.random.seed(42)
torch.manual_seed(42)

n_features = 300
train_features = np.random.randn(len(data['X_train']), n_features).astype(np.float32)
dev_features = np.random.randn(len(data['X_dev']), n_features).astype(np.float32)
test_features = np.random.randn(len(data['X_test']), n_features).astype(np.float32)

# Normalize features
train_features = train_features / (np.linalg.norm(train_features, axis=1, keepdims=True) + 1e-8)
dev_features = dev_features / (np.linalg.norm(dev_features, axis=1, keepdims=True) + 1e-8)
test_features = test_features / (np.linalg.norm(test_features, axis=1, keepdims=True) + 1e-8)

# Convert to tensors
device = torch.device('cpu')
train_features = torch.FloatTensor(train_features).to(device)
train_labels = torch.LongTensor(data['y_train'].numpy()).to(device)
dev_features = torch.FloatTensor(dev_features).to(device)
dev_labels = torch.LongTensor(data['y_dev'].numpy()).to(device)
test_features = torch.FloatTensor(test_features).to(device)
test_labels = torch.LongTensor(data['y_test'].numpy()).to(device)

# Calculate class weights untuk handle imbalanced data
print("Calculating class weights...")
class_weights = compute_class_weight('balanced', classes=np.unique(train_labels.cpu().numpy()), y=train_labels.cpu().numpy())
class_weights = torch.FloatTensor(class_weights).to(device)
print(f"Class weights: {class_weights}")

# Create model
model = ImprovedGNN().to(device)

# Use weighted loss untuk handle imbalance
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)

# Learning rate scheduler
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-6)

# Training loop dengan early stopping
print("\nTraining Improved Model...")
print("="*60)
best_val_acc = 0
patience = 5
patience_counter = 0
num_epochs = 50

for epoch in range(num_epochs):
    # Training
    model.train()
    optimizer.zero_grad()
    outputs = model(train_features)
    loss = criterion(outputs, train_labels)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
    optimizer.step()
    
    # Validation
    model.eval()
    with torch.no_grad():
        dev_outputs = model(dev_features)
        _, dev_predicted = torch.max(dev_outputs, 1)
        train_acc = accuracy_score(train_labels.cpu().numpy(), torch.max(outputs, 1)[1].cpu().numpy())
        val_acc = accuracy_score(dev_labels.cpu().numpy(), dev_predicted.cpu().numpy())
        val_f1 = f1_score(dev_labels.cpu().numpy(), dev_predicted.cpu().numpy(), average='macro')
    
    scheduler.step(val_acc)
    
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        # Save best model
        torch.save(model.state_dict(), 'best_improved_gnn.pth')
    else:
        patience_counter += 1
    
    if (epoch + 1) % 5 == 0:
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")
        print(f"  Loss: {loss.item():.4f}, LR: {scheduler._last_lr[0]:.6f}")
    
    # Early stopping
    if patience_counter >= patience:
        print(f"\nEarly stopping at epoch {epoch+1}")
        break

# Load best model and evaluate on test
print("\n" + "="*60)
print("Evaluating on Test Set")
print("="*60)
model.load_state_dict(torch.load('best_improved_gnn.pth'))
model.eval()
with torch.no_grad():
    test_outputs = model(test_features)
    _, test_predicted = torch.max(test_outputs, 1)
    test_acc = accuracy_score(test_labels.cpu().numpy(), test_predicted.cpu().numpy())
    test_f1 = f1_score(test_labels.cpu().numpy(), test_predicted.cpu().numpy(), average='macro')
    report = classification_report(test_labels.cpu().numpy(), test_predicted.cpu().numpy(),
                                  target_names=['Negative', 'Neutral', 'Positive'], digits=4)

print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test F1-Score (macro): {test_f1:.4f}")
print(f"\nClassification Report:\n{report}")

# Save results
with open('improved_gnn_results.txt', 'w') as f:
    f.write(f"Test Accuracy: {test_acc:.4f}\n")
    f.write(f"Test F1-Score (macro): {test_f1:.4f}\n\n")
    f.write("Classification Report:\n")
    f.write(report)

print("\nâœ… Results saved to improved_gnn_results.txt")

